"""SQL interaction handler with improved caching and validation."""

import logging
from datetime import datetime
from typing import Optional, Dict, Tuple

import pandas as pd
from sqlalchemy import and_, func
from sqlalchemy.exc import IntegrityError, SQLAlchemyError
from sqlalchemy.orm import Session

# If you want to use retries:
from tenacity import retry, stop_after_attempt, wait_exponential

from data.database import StockData
from utils.db_config import get_db_session

logger = logging.getLogger(__name__)

class SQLHandler:
    """Handles SQL database operations with improved caching."""
    
    BATCH_SIZE = 1000
    MAX_RETRIES = 3
    
    def __init__(self):
        self._session: Optional[Session] = None
        # Cache dict uses (symbol, start_date, end_date) -> bool
        self._cache_status: Dict[Tuple[str, datetime, datetime], bool] = {}
        logger.info("ðŸ“Š SQLHandler instance created")
    
    @property
    def session(self) -> Session:
        """Lazily load a database session."""
        if self._session is None or not self._session.is_active:
            self._session = next(get_db_session())
        return self._session

    def _get_cache_key(self, symbol: str, start_date: datetime, end_date: datetime) -> Tuple[str, datetime, datetime]:
        return (symbol, start_date, end_date)

    def is_data_cached(self, symbol: str, start_date: datetime, end_date: datetime) -> bool:
        """
        Check if data is fully cached for the given symbol and date range.
        
        Returns True if the number of records in the DB matches the 
        expected trading days, else False.
        """
        cache_key = self._get_cache_key(symbol, start_date, end_date)
        
        if cache_key in self._cache_status:
            return self._cache_status[cache_key]
        
        # Count how many rows we have in DB for this symbol/date range
        count = self.session.query(func.count(StockData.id)).filter(
            and_(
                StockData.symbol == symbol,
                StockData.date >= start_date,
                StockData.date <= end_date
            )
        ).scalar()
        
        # Approximate the number of trading days using Business day freq
        expected_days = len(pd.date_range(start=start_date, end=end_date, freq='B'))
        
        # Decide if it is "fully cached"
        is_cached = (count == expected_days)
        self._cache_status[cache_key] = is_cached
        
        return is_cached

    def get_cached_data(self, symbol: str, start_date: datetime, end_date: datetime) -> Optional[pd.DataFrame]:
        """
        Retrieve cached data for a symbol in [start_date, end_date].
        Returns a DataFrame or None if no data or if data is incomplete.
        """
        try:
            # If session is left mid-transaction, rollback to avoid errors
            if self.session.in_transaction():
                self.session.rollback()
            
            query = self.session.query(StockData).filter(
                and_(
                    StockData.symbol == symbol,
                    StockData.date >= start_date,
                    StockData.date <= end_date
                )
            ).order_by(StockData.date)
            
            records = query.all()
            if not records:
                logger.info(f"No cached data found for {symbol} in date range {start_date} - {end_date}")
                return None

            # Build DataFrame
            df = pd.DataFrame([
                {
                    'Close': rec.close,
                    'Open': rec.open,
                    'High': rec.high,
                    'Low': rec.low,
                    'Volume': rec.volume,
                    'Date': rec.date
                }
                for rec in records
            ]).set_index('Date')
            
            # Validate data completeness (~90% of trading days is considered "good")
            expected_dates = pd.date_range(start=start_date, end=end_date, freq='B')
            if len(df) < 0.9 * len(expected_dates):
                logger.warning(f"Incomplete cached data for {symbol} â€” only {len(df)} records found.")
                return None

            return df
        
        except SQLAlchemyError as e:
            # Safely rollback if weâ€™re in a broken transaction
            if self.session.is_active:
                self.session.rollback()
            logger.error(f"Database error fetching cached data for {symbol}: {str(e)}")
            return None

    def cache_data(self, symbol: str, data: pd.DataFrame, start_date: datetime, end_date: datetime) -> None:
        """
        Cache data with improved batching. Raises on error so that calling 
        function can decide how to handle the exception.
        """
        try:
            records = []
            
            # Convert DataFrame rows -> list of dicts matching StockData columns
            for date, row in data.iterrows():
                stock_data = {
                    'symbol': symbol,
                    'date': date,
                    'close': row['Close'],
                    'open': row['Open'],
                    'high': row['High'],
                    'low': row['Low'],
                    'volume': row['Volume'],
                    'last_updated': datetime.utcnow()
                }
                records.append(stock_data)

            # Process records in batches
            for i in range(0, len(records), self.BATCH_SIZE):
                batch = records[i:i + self.BATCH_SIZE]
                self._process_batch(batch)

            # If caching is successful, mark in our local cache map
            cache_key = self._get_cache_key(symbol, start_date, end_date)
            self._cache_status[cache_key] = True

        except Exception as e:
            logger.error(f"Error caching data for {symbol}: {str(e)}")
            if self.session.is_active:
                self.session.rollback()
            raise

    @retry(
        stop=stop_after_attempt(MAX_RETRIES),
        wait=wait_exponential(multiplier=1, min=1, max=8),
        reraise=True
    )
    def _process_batch(self, batch: list) -> None:
        """
        Process a batch of records with conflict resolution.
        The @retry decorator will retry up to MAX_RETRIES times with
        exponential backoff if an IntegrityError is raised.
        """
        for record in batch:
            try:
                stock_data = StockData(**record)
                self.session.add(stock_data)
                # Flush to catch IntegrityError on duplicates
                self.session.flush()

            except IntegrityError:
                # We might have a duplicate or conflict
                self.session.rollback()
                self._update_existing_record(record)
                
        # Commit after the whole batch has been processed
        self.session.commit()

    def _update_existing_record(self, record: Dict) -> None:
        """Update an existing record with new data if the row already exists."""
        existing = self.session.query(StockData).filter(
            StockData.symbol == record['symbol'],
            StockData.date == record['date']
        ).first()
        
        if existing:
            existing.open = record['open']
            existing.high = record['high']
            existing.low = record['low']
            existing.close = record['close']
            existing.volume = record['volume']
            existing.last_updated = record['last_updated']
            self.session.commit()

    def _cleanup_session(self):
        """Close the session if open."""
        if self._session is not None:
            try:
                self._session.close()
            except Exception as e:
                logger.warning(f"Error closing session: {str(e)}")
            finally:
                self._session = None

    def __del__(self):
        """Ensure session cleanup on deletion."""
        self._cleanup_session()

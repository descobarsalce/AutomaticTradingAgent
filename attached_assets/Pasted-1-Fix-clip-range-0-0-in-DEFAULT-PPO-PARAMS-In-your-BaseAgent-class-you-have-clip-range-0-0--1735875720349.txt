1. Fix clip_range=0.0 in DEFAULT_PPO_PARAMS

In your BaseAgent class, you have:

'clip_range': 0.0,
This effectively disables the PPO clipping mechanism. Typically, PPO uses something like clip_range=0.1 or 0.2. So, even if you are now using a discrete action space, PPO still expects some clipping in the objective function.

Recommendation: In DEFAULT_PPO_PARAMS, change to something like:
'clip_range': 0.2,
or at least in the [0.1, 0.3] range.
2. Rebalance the Reward Terms in SimpleTradingEnv

Right now, your environment has a very large “holding bonus” and a fairly stiff trading penalty. A typical outcome is that the agent will learn to just hold all the time, especially if that “holding bonus” far outweighs any short-term trading gains.

Quick Fixes
Reduce the holding bonus to something modest. For example:
holding_bonus = 0.5 * self.holding_period
might be too large if you want the agent to trade occasionally. Maybe make it:

holding_bonus = 0.01 * self.holding_period
or even smaller, so that trading can be competitive.
Soften the trading penalty. Right now you do:
if action != 0:
    # ...
    reward = holding_reward * 0.2  # or 0.3 if buying
That’s an 80% penalty (or 70%). Maybe scale it back so the agent only pays a small penalty for trading. For instance:

reward *= 0.8
or another mild approach, instead of a massive multiplier difference.
Consider a small positive reward for “correct” sells. Right now, your reward always punishes selling. If your environment is designed such that “holding always wins,” the agent will not want to close positions at the right times. You can inject logic that checks if you sold at a profit and give a small bonus.
3. Tie the Action “Size” to the Action ID More Clearly (Optional)

Currently, you do the following in your environment:

Buy: 20% of available balance
Sell: 20% of holdings
Hold: do nothing
That is fine as a simple discrete approach. But if you want more granular control or bigger position changes based on repeated “buy” actions, consider:

Action 0: Hold (no change).
Action 1: Buy X% of your total net worth each time.
Action 2: Sell X% of your total holdings each time.
Since you’re already doing that in code, you might confirm that 20% is a good fixed chunk. If the agent seldom has enough steps to ramp up or fully exit, you can tweak it to 50% or 10%, etc. Or you can add more discrete actions (like spaces.Discrete(5)) meaning e.g.:

0 = hold
1 = buy 10%
2 = buy 50%
3 = sell 10%
4 = sell 50%
But that requires a small code refactor.

4. Scale Down Transaction Costs During Training—But Not Too Far

You already do:

self.transaction_cost = transaction_cost * (0.2 if training_mode else 1.0)
This is a good idea (lower cost in training for more exploration).
However, if it’s too low, the policy might learn a trading style that only works under artificially cheap transaction costs. If you want to reduce cost to let the agent explore, maybe do 0.5 or 0.3 instead of 0.2 so it’s not drastically different from reality.
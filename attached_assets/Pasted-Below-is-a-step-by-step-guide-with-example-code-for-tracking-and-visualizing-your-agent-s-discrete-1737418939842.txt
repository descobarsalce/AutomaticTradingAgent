Below is a step-by-step guide (with example code) for tracking and visualizing your agent’s discrete actions over time in a clear, interpretable way. The approach applies to both training and testing. By the end, you will be able to generate a plot that shows when the agent buys (action=1), sells (action=2), or holds (action=0) at each time step (or date).

1. Store Actions Inside the Environment

A simple way to track actions is to record them in the step method of your environment and attach them to info. That way, anyone calling env.step(action) has direct access to the action history.

Modify step to Include Actions in info
Inside SimpleTradingEnv.step():

def step(self, action: Union[int, np.ndarray]) -> Tuple[np.ndarray, float, bool, bool, Dict]:
    ...
    
    # Convert single action to list
    actions = [action] if isinstance(action, (int, np.integer)) else action

    ...
    
    # Process actions
    for idx, symbol in enumerate(self.symbols):
        current_price = float(self.data[symbol].iloc[self.current_step]['Close'])
        discrete_action = int(actions[idx])
        
        # same buy/sell logic
        if discrete_action == 1:  # buy
            ...
        elif discrete_action == 2:  # sell
            ...
        # etc.
    
    ...
    # Attach the raw actions array to info
    info = {
        'net_worth': self.net_worth,
        'balance': self.balance,
        'positions': self.positions.copy(),
        'trades_executed': trades_executed,
        'episode_trades': self.episode_trades.copy(),
        'actions': actions,    # <--- add this line
    }

    return observation, reward, done, truncated, info
For a single asset, actions will be a 1-element list, e.g. [0], [1], or [2].
For multiple assets, actions is something like [1, 0, 2, ...] for each symbol.
2. Collect Actions During Training

If you want to visualize the action sequence during training, you have at least two options:

Use a Custom Callback that runs each step, storing info['actions'].
Modify train() to run your own environment loop. (Sometimes you rely on a stable-baselines style .learn() method that handles the loop internally, in which case a callback is necessary.)
Below is a minimal approach using a custom callback:

from stable_baselines3.common.callbacks import BaseCallback

class ActionLoggingCallback(BaseCallback):
    def __init__(self):
        super().__init__()
        self.actions_history = []

    def _on_step(self) -> bool:
        # The environment is accessible via self.model.env
        # stable-baselines typically vectorizes the env, so we do env.envs[0]
        info = self.model.env.envs[0].get_attr('last_info')[0] 
        # The trick is you need your environment to store the *last info* somewhere.
        # Alternatively, you can patch your environment or callback to directly read the last step's info.
        
        # If you stored the last actions in an attribute
        if info is not None and 'actions' in info:
            self.actions_history.append(info['actions'])
        return True
Alternatively, patch your environment to store the last action attribute:
# Inside SimpleTradingEnv
self.last_info = None

def step(...):
    ...
    info = { ... }
    self.last_info = info
    return observation, reward, done, truncated, info
Then in your custom callback:

info = self.model.env.envs[0].last_info
if info is not None and 'actions' in info:
    self.actions_history.append(info['actions'])
You can save actions_history at the end of training to disk or return it from your training function.

3. Collect Actions During Testing

Your current test() method already stores info_history:

info_history = []

while not done:
    action = self.agent.predict(obs)
    obs, reward, terminated, truncated, info = self.env.step(action)
    done = terminated or truncated
    info_history.append(info)
Each info now contains an actions key (from the updated step method).
After the loop, info_history will look like:
[
  {
    'net_worth': ...,
    'balance': ...,
    'positions': ...,
    'actions': [1],
    ...
  },
  {
    'net_worth': ...,
    'balance': ...,
    'positions': ...,
    'actions': [0],
    ...
  },
  ...
]
You can directly extract the time series of actions from info_history.

4. Plot Discrete Actions Over Time

Here’s a simple utility function using matplotlib that takes info_history and a Pandas date index (if available) to create a scatter plot of actions. It assumes one asset for simplicity; you can easily extend it for multiple assets by creating subplots.

import matplotlib.pyplot as plt

def plot_discrete_actions(info_history, dates=None):
    """
    Creates a scatter plot of discrete actions over time: 
    0 = hold, 1 = buy, 2 = sell.
    Args:
        info_history (List[Dict]): A list of info dicts from environment steps, 
                                   each with an 'actions' key = [discrete_action].
        dates (Optional[List]): A list or array of datetime objects or step indices.
    """
    # Extract actions (assuming single asset => each 'actions' is a list of length 1)
    actions = [d['actions'][0] for d in info_history]  # e.g. [0, 1, 2, 0, 1, ...]

    # If no dates provided, just use step numbers
    if dates is None:
        dates = range(len(actions))
    
    # Create figure
    plt.figure(figsize=(10, 4))
    
    # We'll use a scatter plot with different colors for each discrete action
    color_map = {0: 'blue', 1: 'green', 2: 'red'}
    action_colors = [color_map[a] for a in actions]
    
    # Plot
    plt.scatter(dates, actions, c=action_colors, label='Actions')
    plt.yticks([0, 1, 2], ['Hold(0)', 'Buy(1)', 'Sell(2)'])
    plt.xlabel('Time Step')
    plt.ylabel('Action')
    plt.title('Agent Actions Over Time')
    plt.grid(True)
    plt.show()
Using Actual Date Index
If your DataFrame has a DatetimeIndex, you can pass data.index (shifted by 1 if needed) into plot_discrete_actions:

# In your test method, after collecting info_history, do:
date_index = self.env.data[symbol_name].iloc[1:].index  # or `.iloc[:-1].index`, depends on alignment
plot_discrete_actions(info_history, dates=date_index)
Note: You might need to align your index with the number of steps you actually took in the environment. If you have len(info_history) steps, ensure that date_index has the same length.

5. Putting It All Together

Update SimpleTradingEnv.step() to include actions in the info dictionary.
During training, collect actions either via a callback or direct environment step loop.
During testing, you already accumulate info_history which includes actions.
Plot the stored actions using the utility function (e.g., plot_discrete_actions).
This will let you see how your PPO agent’s discrete decisions evolve over time, e.g.:

Buy signals typically in green
Sell signals in red
Hold signals in blue
You can expand this further with subplots, multi-asset visualizations, or overlaying stock price on the same chart to see if buy/sell decisions align with price movements.

Example Usage in Testing
def test(...):
    ...
    # run your loop
    info_history = []
    while not done:
        action = self.agent.predict(obs)
        obs, reward, terminated, truncated, info = self.env.step(action)
        done = terminated or truncated
        info_history.append(info)
    
    # Now plot actions
    from matplotlib import pyplot as plt
    plot_discrete_actions(info_history,
                          dates=self.env.data[self.symbols[0]].iloc[1:].index)  
    ...
That’s it! You’ll get a scatter plot with the x-axis as time (or step) and the y-axis as the discrete action (0=Hold, 1=Buy, 2=Sell).
Update the learning rate logic to avoid directly passing ExponentialDecay into ppo_params:

def train(self, stock_name: str, start_date: datetime, end_date: datetime,
          env_params: Dict[str, Any], ppo_params: Dict[str, Any],
          callback=None) -> Dict[str, float]:
    data = self.prepare_training_data(stock_name, start_date, end_date)
    self.initialize_env(data, env_params)

    # Configure learning rate schedule
    learning_rate = ppo_params.get('learning_rate', 3e-4)
    decay_steps = ppo_params.get('decay_steps', 1000)
    decay_rate = ppo_params.get('decay_rate', 0.95)

    # Wrap ExponentialDecay in a callable function to evaluate dynamically
    def learning_rate_schedule(step: int):
        return tf.keras.optimizers.schedules.ExponentialDecay(
            initial_learning_rate=learning_rate,
            decay_steps=decay_steps,
            decay_rate=decay_rate
        )(step)

    # Update PPO parameters to pass the initial learning rate (numeric)
    ppo_params['learning_rate'] = learning_rate

    # Initialize agent with updated parameters
    self.agent = TradingAgent(env=self.env, ppo_params=ppo_params)
    self.agent.learning_rate_schedule = learning_rate_schedule

    total_timesteps = (end_date - start_date).days
    self.agent.train(total_timesteps=total_timesteps, callback=callback)
    self.agent.save("trained_model.zip")

    self.portfolio_history = self.env.get_portfolio_history()
    if len(self.portfolio_history) > 1:
        returns = MetricsCalculator.calculate_returns(self.portfolio_history)
        return {
            'sharpe_ratio': MetricsCalculator.calculate_sharpe_ratio(returns),
            'max_drawdown': MetricsCalculator.calculate_maximum_drawdown(self.portfolio_history),
            'sortino_ratio': MetricsCalculator.calculate_sortino_ratio(returns),
            'volatility': MetricsCalculator.calculate_volatility(returns),
            'total_return': (self.portfolio_history[-1] - self.portfolio_history[0]) / self.portfolio_history[0],
            'final_value': self.portfolio_history[-1]
        }
    return {}
TradingAgent Class

Modify the TradingAgent class to apply the learning rate schedule during training if it is set:

class TradingAgent(BaseAgent):
    def __init__(self, env, ppo_params=None, seed=None):
        super().__init__(env, ppo_params=ppo_params, seed=seed)
        self.learning_rate_schedule = None  # Placeholder for dynamic learning rate

    def train(self, total_timesteps: int, callback=None):
        """
        Train the agent with optional dynamic learning rate.
        """
        if self.learning_rate_schedule:
            # Adjust learning rate dynamically before training starts
            self.model.learning_rate = self.learning_rate_schedule(0)

        # Train using the PPO model
        super().train(total_timesteps=total_timesteps, callback=callback)
